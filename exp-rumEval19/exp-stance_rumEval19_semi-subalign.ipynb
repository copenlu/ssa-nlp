{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential rumour stance prediction\n",
    "\n",
    "Rumours tend to change over time; people focus on different aspects of a story.<br>\n",
    "We could view this process as a word space distribution that evolves over time. \n",
    "\n",
    "## Semi-supervised subspace alignment\n",
    "\n",
    "Here we will tackle this problem using a semi-supervised variant of a [Subspace Aligned Classifier](https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Fernando_Unsupervised_Visual_Domain_2013_ICCV_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pc\n",
    "import dateutil\n",
    "import numpy.random as rnd\n",
    "\n",
    "# Import feature extractors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize.api import StringTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Import visualizers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from subalign import SemiSubspaceAlignedClassifier\n",
    "\n",
    "# Import class imbalance techniques\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler, CondensedNearestNeighbour\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set font size\n",
    "fS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental parameters\n",
    "which_rumour = 'charliehebdo'\n",
    "classifier = 'lr'\n",
    "kernel = 'rbf'\n",
    "degree = 3\n",
    "l2 = 1.0\n",
    "subspace_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X = np.load('../data/RumEval2019/rumeval19.npy')\n",
    "tweets = pd.read_json('../data/RumEval2019/RumEval19.json')\n",
    "\n",
    "X[np.isnan(X)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rumours\n",
    "rumours_ = tweets['rumour'].tolist()\n",
    "rumours = np.unique(tweets['rumour'].tolist())\n",
    "[print(rumour) for rumour in rumours];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dates\n",
    "# dates_ = [dateutil.parser.parse(date).date() for date in tweets['date'].tolist()]\n",
    "dates_ = [date.date() for date in tweets['date'].tolist()]\n",
    "dates = np.unique(dates_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential prediction over days within 1 rumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect rumour 'charliehebdo'\n",
    "tweets_r = tweets[tweets['rumour'] == which_rumour]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect embeddings\n",
    "X = X[(tweets['rumour'] == which_rumour).values, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect labels and map to numerical\n",
    "labels_ = tweets_r['label'].values.tolist()\n",
    "labels = np.unique(labels_)\n",
    "Y = np.array([np.argwhere(label == labels)[0][0] for label in labels_])\n",
    "K = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect dates\n",
    "dates_ = tweets_r['date'].values\n",
    "dates = np.unique(tweets_r['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification experiment\n",
    "\n",
    "# Preallocate performance array\n",
    "nperf_array = []\n",
    "aperf_array = []\n",
    "days_array = []\n",
    "sub_dim_d = []\n",
    "\n",
    "# Loop over days\n",
    "for d in range(1, len(dates)-2):\n",
    "\n",
    "    # Set range up to yesterday\n",
    "    past = range(d)\n",
    "\n",
    "    # Create training data from all previous days\n",
    "    trn_index = tweets_r['date'].isin(dates[past]).values\n",
    "\n",
    "    # Find all tweets from today\n",
    "    tst_index = (tweets_r['date'] == dates[d]).values.tolist()\n",
    "\n",
    "    # Split out training data\n",
    "    trn_X = X[trn_index, :]\n",
    "    trn_Y = Y[trn_index]\n",
    "\n",
    "    # Split out test data\n",
    "    tst_X = X[tst_index, :]\n",
    "    tst_Y = Y[tst_index]\n",
    "    \n",
    "    # Set up under-sampling using specified classifier\n",
    "    trn_X, trn_Y = RandomOverSampler().fit_resample(trn_X, trn_Y)\n",
    "    \n",
    "    # Check label proportions\n",
    "    print('Label shape of train set, after resampling: \\t %s' % Counter(trn_Y))\n",
    "    \n",
    "    # Sample size\n",
    "    N = trn_X.shape[0]\n",
    "    M = tst_X.shape[0]\n",
    "    \n",
    "    # Random selection of 1 target label per class for semi-supervision\n",
    "    u = np.zeros((K, 2), dtype='uint8')\n",
    "    for k in range(K):\n",
    "        u[k, 0] = rnd.choice(np.arange(M), size=1, p=(tst_Y == k) / np.sum(tst_Y == k))\n",
    "        u[k, 1] = k\n",
    "        \n",
    "        # Subspace dimensionality for today\n",
    "    sub_dim_d.append(min(subspace_dim, min(N, M)))\n",
    "    \n",
    "    # Adaptive classifier\n",
    "    aclf = SemiSubspaceAlignedClassifier()\n",
    "    V, CX, CZ = aclf.semi_subspace_alignment(trn_X, trn_Y, tst_X, u, subspace_dim=min(subspace_dim, min(N, M)))\n",
    "    \n",
    "    # Define classifier\n",
    "    if classifier in ('lr', 'logr', 'logistic'):\n",
    "        \n",
    "        # Linear logistic model\n",
    "        nclf = linear_model.LogisticRegression(C=l2, multi_class='auto', solver='lbfgs')\n",
    "        aclf = linear_model.LogisticRegression(C=l2, multi_class='auto', solver='lbfgs')\n",
    "        \n",
    "    elif classifier in ('svm', 'svc', 'rbfsvc'):\n",
    "        \n",
    "        # Polynomial support vector machine\n",
    "        nclf = svm.SVC(kernel=kernel, gamma='auto', C=l2)\n",
    "        aclf = svm.SVC(kernel=kernel, gamma='auto', C=l2)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Classifier not recognized')\n",
    "\n",
    "    # Train classifier on labeled data from previous days\n",
    "    nclf.fit(trn_X, trn_Y)\n",
    "    \n",
    "    trn_XV = np.zeros((N, V[k].shape[1]))\n",
    "    for k in range(K):\n",
    "        trn_XV[trn_Y == k, :] = trn_X[trn_Y == k, :] @ CX[k] @ V[k]\n",
    "    \n",
    "    # Train classifier on labeled data from previous days, aligned to unlabeled data from today\n",
    "    aclf.fit(trn_XV, trn_Y)\n",
    "\n",
    "    # Make predictions on test set\n",
    "    npreds = nclf.predict(tst_X)\n",
    "    apreds = aclf.predict(tst_X @ CZ)\n",
    "\n",
    "    # Test on data from current day and store\n",
    "    nperf_array.append(np.mean(npreds == tst_Y))\n",
    "    aperf_array.append(np.mean(apreds == tst_Y))\n",
    "\n",
    "# Compact to DataFrame\n",
    "results = pd.DataFrame({'dates': dates[1:-2],\n",
    "                        'acc_naive': nperf_array,\n",
    "                        'acc_adaptive': aperf_array,\n",
    "                        'subspace_dim': sub_dim_d},\n",
    "                        columns=['dates', 'acc_naive', 'acc_adaptive', 'subspace_dim'])    \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance in bar graph\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Bar plot\n",
    "results.plot.bar(ax=ax, x='dates', y=[\"acc_naive\", \"acc_adaptive\"], rot=30, fontsize=fS, legend=True);\n",
    "\n",
    "# Set axes properties\n",
    "ax.set_ylim([0, 1]);\n",
    "ax.legend(fontsize=fS, loc=2);\n",
    "ax.set_xlabel('Days', fontsize=fS);\n",
    "ax.set_ylabel('Accuracy', fontsize=fS);\n",
    "\n",
    "fig.savefig('exp-stance_rumEval19_subalign_bar-dates.png', bbox_inches='tight', padding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure\n",
    "fig, ax = plt.subplots(nrows=len(dates)-3, ncols=3, sharex=True, sharey=True, figsize=(30, 40))\n",
    "\n",
    "# Loop over days\n",
    "for d, date in enumerate(dates[:-2]):\n",
    "    \n",
    "    if d == 0:\n",
    "        continue\n",
    "    \n",
    "    # Set range up to yesterday\n",
    "    past = range(d)\n",
    "\n",
    "    # Create training data from all previous days\n",
    "    trn_index = tweets_r['date'].isin(dates[past]).values\n",
    "\n",
    "    # Find all tweets from today\n",
    "    tst_index = (tweets_r['date'] == dates[d]).values.tolist()\n",
    "  \n",
    "    # Split out training data\n",
    "    trn_X = X[trn_index, :]\n",
    "    trn_Y = Y[trn_index]\n",
    "\n",
    "    # Split out test data\n",
    "    tst_X = X[tst_index, :]\n",
    "    tst_Y = Y[tst_index]\n",
    "    \n",
    "    # Find subspace\n",
    "    clf = SemiSubspaceAlignedClassifier()\n",
    "    V, CX, CZ = clf.semi_subspace_alignment(trn_X, trn_Y, tst_X, u, subspace_dim=2)\n",
    "    \n",
    "    X_ = np.zeros((trn_X.shape[0], CX[k].shape[1]))\n",
    "    XV = np.zeros((trn_X.shape[0], V[k].shape[1]))\n",
    "    for k in range(K):\n",
    "        X_[trn_Y == k, :] = trn_X[trn_Y == k, :] @ CX[k]\n",
    "        XV[trn_Y == k, :] = trn_X[trn_Y == k, :] @ CX[k] @ V[k]\n",
    "    \n",
    "    # Map target data onto components \n",
    "    Z_ = tst_X @ CZ\n",
    "    \n",
    "    for l, label in enumerate(labels):\n",
    "\n",
    "        ax[d-1][0].scatter(X_[(trn_Y == l), 0], X_[(trn_Y == l), 1], label=label)\n",
    "        ax[d-1][0].set_title('Past dates')\n",
    "        ax[d-1][0].set_xlabel('p_1')\n",
    "        ax[d-1][0].set_ylabel('p_2')\n",
    "        ax[d-1][0].legend(fontsize=fS)\n",
    "        \n",
    "        ax[d-1][1].scatter(XV[(trn_Y == l), 0], XV[(trn_Y == l), 1], label=label)\n",
    "        ax[d-1][1].set_title('Transformed')\n",
    "        ax[d-1][1].set_xlabel('p_1')\n",
    "        ax[d-1][1].set_ylabel('p_2')\n",
    "        ax[d-1][1].legend(fontsize=fS)\n",
    "        \n",
    "        ax[d-1][2].scatter(Z_[(tst_Y == l), 0], Z_[(tst_Y == l), 1], label=label)\n",
    "        ax[d-1][2].set_title(str(date)[:10])\n",
    "        ax[d-1][2].set_xlabel('p_1')\n",
    "        ax[d-1][2].set_ylabel('p_2')\n",
    "        ax[d-1][2].legend(fontsize=fS)\n",
    "    \n",
    "plt.savefig('exp-stance_rumEval19_subalign_days-charliehebdo.png', bbox_inches=None, padding='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential prediction over rumours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X = np.load('../data/RumEval2019/rumeval19.npy')\n",
    "tweets = pd.read_json('../data/RumEval2019/RumEval19.json')\n",
    "\n",
    "X[np.isnan(X)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect labels and map to numerical\n",
    "labels_ = tweets['label'].values.tolist()\n",
    "labels = np.unique(labels_)\n",
    "Y = np.array([np.argwhere(label == labels)[0][0] for label in labels_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.sort_values(by=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_ = tweets['date'].values\n",
    "dates = np.unique(tweets['date'].values)\n",
    "sortix = np.argsort(dates_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rumours_ = tweets['rumour'].values[sortix]\n",
    "indexes = np.unique(rumours_, return_index=True)[1]\n",
    "all_rumours = [rumours_[index] for index in sorted(indexes)]\n",
    "\n",
    "# Remove rumours with too few samples\n",
    "cutoff = subspace_dim\n",
    "rumours = []\n",
    "for rumour in all_rumours:\n",
    "    \n",
    "    # Number of samples for current rumour\n",
    "    num_rumour = np.sum(rumours_ == rumour)\n",
    "    \n",
    "    if num_rumour >= cutoff:\n",
    "        rumours.append(rumour)\n",
    "        \n",
    "print('{} rumours discarded for having less than {} samples.\\n'.format(len(all_rumours) - len(rumours), cutoff))\n",
    "\n",
    "print('Remaining rumours:')\n",
    "[print(rumour) for rumour in rumours];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort embedding\n",
    "X = X[sortix, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of repetitions of the experiment\n",
    "nI = 1\n",
    "\n",
    "# Preallocate performance array\n",
    "nperf_array = np.zeros((nR-2, nI))\n",
    "aperf_array = np.zeros((nR-2, nI))\n",
    "sub_dim_r = np.zeros((nR-2, nI))\n",
    "\n",
    "# Start iterating\n",
    "for n in range(10):\n",
    "    \n",
    "    print('At repetition {}/{}'.format(n, nI))\n",
    "\n",
    "    # Start looping over rumours\n",
    "    for r in range(1, len(rumours)-1):\n",
    "\n",
    "        print('At rumour ' + rumours[r])\n",
    "\n",
    "        # Set range up to yesterday\n",
    "        past = slice(0,r)\n",
    "\n",
    "        # Create training data from all previous days\n",
    "        trn_index = tweets['rumour'].isin(rumours[past]).values\n",
    "\n",
    "        # Find all tweets from today\n",
    "        tst_index = (tweets['rumour'] == rumours[r]).values.tolist()\n",
    "\n",
    "        # Split out training data\n",
    "        trn_X = X[trn_index, :]\n",
    "        trn_Y = Y[trn_index]\n",
    "\n",
    "        # Split out test data\n",
    "        tst_X = X[tst_index, :]\n",
    "        tst_Y = Y[tst_index]\n",
    "\n",
    "        # Set up under-sampling using specified classifier\n",
    "        trn_X, trn_Y = RandomOverSampler().fit_resample(trn_X, trn_Y)\n",
    "\n",
    "        # Check label proportions\n",
    "        print('Label shape of train set, after resampling: \\t %s' % Counter(trn_Y))\n",
    "\n",
    "        # Sample size\n",
    "        N = trn_X.shape[0]\n",
    "        M = tst_X.shape[0]\n",
    "\n",
    "        # Subspace dimensionality for today\n",
    "        if n == 0:\n",
    "            sub_dim_r[r-1] = min(subspace_dim, min(N, M))\n",
    "\n",
    "        # Random selection of 1 target label per class for semi-supervision\n",
    "        u = np.zeros((K, 2), dtype='uint8')\n",
    "        for k in range(K):\n",
    "            u[k, 0] = rnd.choice(np.arange(M), size=1, p=(tst_Y == k) / np.sum(tst_Y == k))\n",
    "            u[k, 1] = k\n",
    "\n",
    "        # Adaptive classifier\n",
    "        aclf = SubspaceAlignedClassifier()\n",
    "        V, CX, CZ = aclf.semi_subspace_alignment(trn_X, trn_Y, tst_X, u, subspace_dim=min(subspace_dim, min(N, M)))\n",
    "\n",
    "        # Check if subspace was reduced\n",
    "        if n == 0:\n",
    "            sub_dim_r[r-1] = V[k].shape[1]\n",
    "\n",
    "        # Define classifier\n",
    "        if classifier in ('lr', 'logr', 'logistic'):\n",
    "\n",
    "            # Linear logistic model\n",
    "            nclf = linear_model.LogisticRegression(C=l2, multi_class='auto', solver='lbfgs')\n",
    "            aclf = linear_model.LogisticRegression(C=l2, multi_class='auto', solver='lbfgs')\n",
    "\n",
    "        elif classifier in ('svm', 'svc', 'rbfsvc'):\n",
    "\n",
    "            # Polynomial support vector machine\n",
    "            nclf = svm.SVC(kernel=kernel, gamma='auto', C=l2)\n",
    "            aclf = svm.SVC(kernel=kernel, gamma='auto', C=l2)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Classifier not recognized')\n",
    "\n",
    "        # Train classifier on labeled data from previous days\n",
    "        nclf.fit(trn_X, trn_Y)\n",
    "\n",
    "        trn_XV = np.zeros((N, V[k].shape[1]))\n",
    "        for k in range(K):\n",
    "            trn_XV[trn_Y == k, :] = trn_X[trn_Y == k, :] @ CX[k] @ V[k]\n",
    "\n",
    "        # Train classifier on labeled data from previous days, aligned to unlabeled data from today\n",
    "        aclf.fit(trn_XV, trn_Y)\n",
    "\n",
    "        # Make predictions on test set\n",
    "        npreds = nclf.predict(tst_X)\n",
    "        apreds = aclf.predict(tst_X @ CZ)\n",
    "\n",
    "        # Test on data from current day and store\n",
    "        nperf_array[r-1, n] = np.mean(npreds == tst_Y)\n",
    "        aperf_array[r-1, n] = np.mean(apreds == tst_Y)\n",
    "\n",
    "# Compact to DataFrame\n",
    "rum_results = pd.DataFrame({'rumours': rumours[1:-1],\n",
    "                            'acc_naive': np.mean(nperf_array, axis=1),\n",
    "                            'acc_adaptive': np.mean(aperf_array, axis=1),\n",
    "                            'subspace_dim': sub_dim_r},\n",
    "                           columns=['rumours', 'acc_naive', 'acc_adaptive', 'sub_dim_r'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of repetitions of the experiment\n",
    "nI = 1\n",
    "\n",
    "# Preallocate performance array\n",
    "nperf_array = np.zeros((nR-2, nI))\n",
    "aperf_array = np.zeros((nR-2, nI))\n",
    "sub_dim_r = np.zeros((nR-2, nI))\n",
    "\n",
    "# Start iterating\n",
    "for n in range(10):\n",
    "    \n",
    "    print('At repetition {}/{}'.format(n, nI))\n",
    "\n",
    "    # Start looping over rumours\n",
    "    for r in range(1, len(rumours)-1):\n",
    "\n",
    "        print('At rumour ' + rumours[r])\n",
    "\n",
    "        # Set range up to yesterday\n",
    "        past = slice(0,r)\n",
    "\n",
    "        # Create training data from all previous days\n",
    "        trn_index = tweets['rumour'].isin(rumours[past]).values\n",
    "\n",
    "        # Find all tweets from today\n",
    "        tst_index = (tweets['rumour'] == rumours[r]).values.tolist()\n",
    "\n",
    "        # Split out training data\n",
    "        trn_X = X[trn_index, :]\n",
    "        trn_Y = Y[trn_index]\n",
    "\n",
    "        # Split out test data\n",
    "        tst_X = X[tst_index, :]\n",
    "        tst_Y = Y[tst_index]\n",
    "\n",
    "        # Set up under-sampling using specified classifier\n",
    "        trn_X, trn_Y = RandomOverSampler().fit_resample(trn_X, trn_Y)\n",
    "\n",
    "        # Check label proportions\n",
    "        print('Label shape of train set, after resampling: \\t %s' % Counter(trn_Y))\n",
    "\n",
    "        # Sample size\n",
    "        N = trn_X.shape[0]\n",
    "        M = tst_X.shape[0]\n",
    "\n",
    "        # Subspace dimensionality for today\n",
    "        if n == 0:\n",
    "            sub_dim_r[r-1] = min(subspace_dim, min(N, M))\n",
    "\n",
    "        # Random selection of 1 target label per class for semi-supervision\n",
    "        u = np.zeros((K, 2), dtype='uint8')\n",
    "        for k in range(K):\n",
    "            u[k, 0] = rnd.choice(np.arange(M), size=1, p=(tst_Y == k) / np.sum(tst_Y == k))\n",
    "            u[k, 1] = k\n",
    "\n",
    "        # Adaptive classifier\n",
    "        aclf = SubspaceAlignedClassifier()\n",
    "        V, CX, CZ = aclf.semi_subspace_alignment(trn_X, trn_Y, tst_X, u, subspace_dim=min(subspace_dim, min(N, M)))\n",
    "\n",
    "        # Check if subspace was reduced\n",
    "        if n == 0:\n",
    "            sub_dim_r[r-1] = V[k].shape[1]\n",
    "\n",
    "        # Define classifier\n",
    "        if classifier in ('lr', 'logr', 'logistic'):\n",
    "\n",
    "            # Linear logistic model\n",
    "            nclf = linear_model.LogisticRegression(C=l2, multi_class='auto', solver='lbfgs')\n",
    "            aclf = linear_model.LogisticRegression(C=l2, multi_class='auto', solver='lbfgs')\n",
    "\n",
    "        elif classifier in ('svm', 'svc', 'rbfsvc'):\n",
    "\n",
    "            # Polynomial support vector machine\n",
    "            nclf = svm.SVC(kernel=kernel, gamma='auto', C=l2)\n",
    "            aclf = svm.SVC(kernel=kernel, gamma='auto', C=l2)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Classifier not recognized')\n",
    "\n",
    "        # Train classifier on labeled data from previous days\n",
    "        nclf.fit(trn_X, trn_Y)\n",
    "\n",
    "        trn_XV = np.zeros((N, V[k].shape[1]))\n",
    "        for k in range(K):\n",
    "            trn_XV[trn_Y == k, :] = trn_X[trn_Y == k, :] @ CX[k] @ V[k]\n",
    "\n",
    "        # Train classifier on labeled data from previous days, aligned to unlabeled data from today\n",
    "        aclf.fit(trn_XV, trn_Y)\n",
    "\n",
    "        # Make predictions on test set\n",
    "        npreds = nclf.predict(tst_X)\n",
    "        apreds = aclf.predict(tst_X @ CZ)\n",
    "\n",
    "        # Test on data from current day and store\n",
    "        nperf_array[r-1, n] = np.mean(npreds == tst_Y)\n",
    "        aperf_array[r-1, n] = np.mean(apreds == tst_Y)\n",
    "\n",
    "# Compact to DataFrame\n",
    "rum_results = pd.DataFrame({'rumours': rumours[1:-1],\n",
    "                            'acc_naive': np.mean(nperf_array, axis=1),\n",
    "                            'acc_adaptive': np.mean(aperf_array, axis=1),\n",
    "                            'subspace_dim': sub_dim_r},\n",
    "                           columns=['rumours', 'acc_naive', 'acc_adaptive', 'sub_dim_r'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=len(mappings), figsize=(15,50))\n",
    "\n",
    "for m, mapping in enumerate(mappings):\n",
    "    \n",
    "    im = ax[m].imshow(mapping)\n",
    "    ax[m].get_xaxis().set_visible(False)\n",
    "    ax[m].get_yaxis().set_visible(False)\n",
    "    plt.colorbar(im, ax=ax[m])\n",
    "    ax[m].set_title(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance in bar graph\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Bar plot\n",
    "rum_results.plot.bar(ax=ax, x='rumours', y=[\"acc_naive\", \"acc_adaptive\"], rot=30, fontsize=fS, legend=True);\n",
    "\n",
    "# Set axes properties\n",
    "ax.set_ylim([0, 1]);\n",
    "ax.legend(fontsize=fS, loc=2);\n",
    "ax.set_xlabel('Days', fontsize=fS);\n",
    "ax.set_ylabel('Accuracy', fontsize=fS);\n",
    "\n",
    "fig.savefig('exp-stance_rumEval19_subalign_bar-rumours.png', bbox_inches='tight', padding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure\n",
    "fig, ax = plt.subplots(nrows=len(rumours)-2, ncols=3, sharex=True, sharey=True, figsize=(30, 40))\n",
    "\n",
    "# Start looping over rumours\n",
    "for r in range(1, len(rumours)-1):\n",
    "    \n",
    "    print('At rumour ' + rumours[r])\n",
    "    \n",
    "    # Set range up to yesterday\n",
    "    past = slice(0,r)\n",
    "\n",
    "    # Create training data from all previous days\n",
    "    trn_index = tweets['rumour'].isin(rumours[past]).values\n",
    "\n",
    "    # Find all tweets from today\n",
    "    tst_index = (tweets['rumour'] == rumours[r]).values.tolist()\n",
    "\n",
    "    # Split out training data\n",
    "    trn_X = X[trn_index, :]\n",
    "    trn_Y = Y[trn_index]\n",
    "\n",
    "    # Split out test data\n",
    "    tst_X = X[tst_index, :]\n",
    "    tst_Y = Y[tst_index]\n",
    "    \n",
    "    # Find subspace\n",
    "    clf = SubspaceAlignedClassifier(subspace_dim=2)\n",
    "    V, CX, CZ = clf.semi_subspace_alignment(trn_X, trn_Y, tst_X, u, subspace_dim=2)\n",
    "    \n",
    "    X_ = np.zeros((trn_X.shape[0], CX[k].shape[1]))\n",
    "    XV = np.zeros((trn_X.shape[0], V[k].shape[1]))\n",
    "    for k in range(K):\n",
    "        X_[trn_Y == k, :] = trn_X[trn_Y == k, :] @ CX[k]\n",
    "        XV[trn_Y == k, :] = trn_X[trn_Y == k, :] @ CX[k] @ V[k]\n",
    "    \n",
    "    # Map target data onto components \n",
    "    Z_ = tst_X @ CZ\n",
    "    \n",
    "    for l, label in enumerate(labels):\n",
    "\n",
    "        ax[r-1][0].scatter(X_[(trn_Y == l), 0], X_[(trn_Y == l), 1], label=label)\n",
    "        ax[r-1][0].set_title('Past rumours (' + ', '.join(rumours[past]) + ')')\n",
    "        ax[r-1][0].set_xlabel('p_1')\n",
    "        ax[r-1][0].set_ylabel('p_2')\n",
    "        ax[r-1][0].legend(fontsize=fS)\n",
    "        \n",
    "        ax[r-1][1].scatter(XV[(trn_Y == l), 0], XV[(trn_Y == l), 1], label=label)\n",
    "        ax[r-1][1].set_title('Transformed')\n",
    "        ax[r-1][1].set_xlabel('p_1')\n",
    "        ax[r-1][1].set_ylabel('p_2')\n",
    "        ax[r-1][1].legend(fontsize=fS)\n",
    "        \n",
    "        ax[r-1][2].scatter(Z_[(tst_Y == l), 0], Z_[(tst_Y == l), 1], label=label)\n",
    "        ax[r-1][2].set_title(rumours[r])\n",
    "        ax[r-1][2].set_xlabel('p_1')\n",
    "        ax[r-1][2].set_ylabel('p_2')\n",
    "        ax[r-1][2].legend(fontsize=fS)\n",
    "    \n",
    "plt.savefig('exp-stance_rumEval19_subalign_rumours.png', bbox_inches=None, padding='tight')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential prediction over days for all rumours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect labels and map to numerical\n",
    "labels_ = tweets['label'].values.tolist()\n",
    "labels = np.unique(labels_)\n",
    "Y = np.array([np.argwhere(label == labels)[0][0] for label in labels_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dates\n",
    "dates_ = tweets['date'].values\n",
    "\n",
    "all_dates = np.unique(dates_)\n",
    "\n",
    "# Remove dates with too few samples\n",
    "cutoff = 10\n",
    "dates = []\n",
    "for date in all_dates:\n",
    "    \n",
    "    # Number of samples for current date\n",
    "    num_date = np.sum(dates_ == date)\n",
    "    \n",
    "    if num_date > cutoff:\n",
    "        dates.append(date)\n",
    "        \n",
    "print('{} dates discarded for having less than {} samples.'.format(len(all_dates) - len(dates), cutoff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it would be a good idea to forget older rumours. Like an exponentially decaying weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification experiment\n",
    "\n",
    "# Preallocate performance array\n",
    "nperf_array = []\n",
    "aperf_array = []\n",
    "days_array = []\n",
    "sub_dim_d = []\n",
    "mappings = []\n",
    "\n",
    "# Loop over days\n",
    "for d in range(1, len(dates)-1):\n",
    "\n",
    "    # Set range up to yesterday\n",
    "    past = range(d)\n",
    "\n",
    "    # Create training data from all previous days\n",
    "    trn_index = tweets['date'].isin(dates[past]).values\n",
    "\n",
    "    # Find all tweets from today\n",
    "    tst_index = (tweets['date'] == dates[d]).values.tolist()\n",
    "\n",
    "    # Split out training data\n",
    "    trn_X = X[trn_index, :]\n",
    "    trn_Y = Y[trn_index]\n",
    "\n",
    "    # Split out test data\n",
    "    tst_X = X[tst_index, :]\n",
    "    tst_Y = Y[tst_index]\n",
    "    \n",
    "    # Set up under-sampling using specified classifier\n",
    "    trn_X_, trn_Y_ = RandomOverSampler().fit_resample(trn_X, trn_Y)\n",
    "    \n",
    "    # Check label proportions\n",
    "    print('Label shape of train set, after resampling: \\t %s' % Counter(trn_Y_))\n",
    "    \n",
    "    # Sample size\n",
    "    N = trn_X.shape[0]\n",
    "    M = tst_X.shape[0]\n",
    "    \n",
    "    # Store subspace dimensionality\n",
    "    sub_dim_d.append(min(subspace_dim, min(N, M)))\n",
    "    \n",
    "    # Adaptive classifier\n",
    "    aclf = SubspaceAlignedClassifier()\n",
    "    V, CX, CZ = aclf.subspace_alignment(trn_X_, tst_X, subspace_dim=min(subspace_dim, min(N, M)))\n",
    "    \n",
    "    # Store mappings\n",
    "    mappings.append(V)\n",
    "    \n",
    "    # Define classifier\n",
    "    if classifier in ('lr', 'logr', 'logistic'):\n",
    "        \n",
    "        # Linear logistic model\n",
    "        nclf = linear_model.LogisticRegression(C=l2, multi_class='auto', solver='lbfgs')\n",
    "        aclf = linear_model.LogisticRegression(C=l2, multi_class='auto', solver='lbfgs')\n",
    "        \n",
    "    elif classifier in ('svm', 'svc', 'rbfsvc'):\n",
    "        \n",
    "        # Polynomial support vector machine\n",
    "        nclf = svm.SVC(kernel=kernel, gamma='auto', C=l2)\n",
    "        aclf = svm.SVC(kernel=kernel, gamma='auto', C=l2)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Classifier not recognized')\n",
    "\n",
    "    # Train classifier on labeled data from previous days\n",
    "    nclf.fit(trn_X_, trn_Y_)\n",
    "    \n",
    "    # Train classifier on labeled data from previous days, aligned to unlabeled data from today\n",
    "    aclf.fit(trn_X_ @ CX @ V, trn_Y_)\n",
    "\n",
    "    # Make predictions on test set\n",
    "    npreds = nclf.predict(tst_X)\n",
    "    apreds = aclf.predict(tst_X @ CZ)\n",
    "\n",
    "    # Test on data from current day and store\n",
    "    nperf_array.append(np.mean(npreds == tst_Y))\n",
    "    aperf_array.append(np.mean(apreds == tst_Y))\n",
    "\n",
    "# Compact to DataFrame\n",
    "results = pd.DataFrame({'dates': dates[1:-1],\n",
    "                        'acc_naive': nperf_array,\n",
    "                        'acc_adaptive': aperf_array,\n",
    "                        'subspace_dim': sub_dim_d},\n",
    "                        columns=['dates', 'acc_naive', 'acc_adaptive', 'subspace_dim'])    \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance in bar graph\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(figsize=(15, 50))\n",
    "\n",
    "# Bar plot\n",
    "results.plot.barh(ax=ax, x='dates', y=[\"acc_naive\", \"acc_adaptive\"], fontsize=fS, legend=True);\n",
    "\n",
    "# Set axes properties\n",
    "# ax.set_ylim([0, 1]);\n",
    "ax.legend(fontsize=fS, loc=1);\n",
    "ax.set_ylabel('Days', fontsize=fS);\n",
    "ax.set_xlabel('Accuracy', fontsize=fS);\n",
    "\n",
    "fig.savefig('exp-stance_rumEval19_subalign_bar-dates-allrumours.png', bbox_inches='tight', padding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
